{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47a1f73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.optimize import nnls\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ---------- Load & index ----------\n",
    "df = pd.read_csv(\"kaggle_clustering_student_version.csv\")\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "df[\"date\"]      = df[\"timestamp\"].dt.date\n",
    "df[\"hour\"]      = df[\"timestamp\"].dt.hour\n",
    "df[\"dow\"]       = df[\"timestamp\"].dt.dayofweek\n",
    "df[\"is_weekend\"]= df[\"dow\"] >= 5\n",
    "df[\"month\"]     = df[\"timestamp\"].dt.month\n",
    "df[\"kaggle_id\"] = df[\"household_id\"].astype(str) + \"_\" + df[\"timestamp\"].dt.strftime(\"%Y-%m-%d %H\")\n",
    "\n",
    "# ---------- Handy constants ----------\n",
    "NIGHT_HOURS = [2,3,4,5]\n",
    "EV_WD_HOURS = [20,21,22,23,0,1]\n",
    "EV_WE_HOURS = [11,12,13,14,15,16]\n",
    "HEAT_MORN   = [6,7,8,9]\n",
    "HEAT_EVE    = [17,18,19,20,21]\n",
    "MAY_MONTH   = 5\n",
    "\n",
    "def safe_pivot(pdf, value_col):\n",
    "    # Robust pivot: average duplicates, keep all hours, drop incomplete days\n",
    "    agg = (pdf.groupby([\"date\",\"hour\"], as_index=False)[value_col].mean())\n",
    "    mat = (agg.pivot(index=\"date\", columns=\"hour\", values=value_col)\n",
    "              .reindex(columns=range(24)))\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ce4b4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v4/lzkm2zzd3gb0q8rqggx5drch0000gn/T/ipykernel_13921/677769277.py:17: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  baseline_df = df.groupby(\"household_id\").apply(pick_baseline_for_house).reset_index()\n"
     ]
    }
   ],
   "source": [
    "# Try a few quantiles per household; pick the one minimizing *night* residual MSE\n",
    "BASELINE_QS = [0.10, 0.15, 0.20]\n",
    "\n",
    "def pick_baseline_for_house(pdf):\n",
    "    best_q, best_b, best_err = None, None, 1e9\n",
    "    for q in BASELINE_QS:\n",
    "        night_vals = pdf.loc[pdf[\"hour\"].isin(NIGHT_HOURS), \"consumption_kWh\"].values\n",
    "        if len(night_vals) < 4:\n",
    "            night_vals = pdf[\"consumption_kWh\"].values\n",
    "        b = float(np.quantile(night_vals, q))\n",
    "        b = max(b, 0.01)\n",
    "        err = np.mean(np.clip(night_vals - b, 0, None)**2)  # night residual MSE\n",
    "        if err < best_err:\n",
    "            best_q, best_b, best_err = q, b, err\n",
    "    return pd.Series({\"baseline_kWh\": best_b, \"q\": best_q})\n",
    "\n",
    "baseline_df = df.groupby(\"household_id\").apply(pick_baseline_for_house).reset_index()\n",
    "baseline_map = dict(zip(baseline_df[\"household_id\"], baseline_df[\"baseline_kWh\"]))\n",
    "\n",
    "df[\"always_on_kWh\"] = df[\"household_id\"].map(baseline_map)\n",
    "df[\"residual_kWh\"]  = (df[\"consumption_kWh\"] - df[\"always_on_kWh\"]).clip(lower=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "199020ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline describe:\n",
      " count    200.000000\n",
      "mean       0.235872\n",
      "std        0.092814\n",
      "min        0.078180\n",
      "25%        0.167225\n",
      "50%        0.225470\n",
      "75%        0.285400\n",
      "max        0.559780\n",
      "Name: baseline_kWh, dtype: float64\n",
      "Night residual kWh — mean: 0.37791210271739134  median: 0.2062\n",
      "Share of hours with baseline > total (should be < 1%): 16.74%\n"
     ]
    }
   ],
   "source": [
    "# 1) Distribution of baselines\n",
    "b = baseline_df[\"baseline_kWh\"].describe()\n",
    "print(\"Baseline describe:\\n\", b)\n",
    "\n",
    "# 2) Night residual should be small (median ~0.0–0.05)\n",
    "night = df[df[\"hour\"].isin(NIGHT_HOURS)]\n",
    "night_resid = (night[\"consumption_kWh\"] - night[\"always_on_kWh\"]).clip(lower=0)\n",
    "print(\"Night residual kWh — mean:\", night_resid.mean(), \" median:\", night_resid.median())\n",
    "\n",
    "# 3) Sanity: share of hours where baseline > consumption (should be tiny)\n",
    "neg_share = (df[\"consumption_kWh\"] < df[\"always_on_kWh\"]).mean()\n",
    "print(\"Share of hours with baseline > total (should be < 1%):\", f\"{100*neg_share:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13537f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EV owners detected: 55\n"
     ]
    }
   ],
   "source": [
    "def window_stats(pdf, hours):\n",
    "    mat = safe_pivot(pdf, \"residual_kWh\")\n",
    "    if mat is None or mat.empty:\n",
    "        return np.zeros(6)\n",
    "    vals = mat[hours].fillna(0.0).values\n",
    "    day_sum = vals.sum(axis=1)\n",
    "    day_p95 = np.percentile(vals, 95, axis=1)\n",
    "    day_max = vals.max(axis=1)\n",
    "    # hours above fixed levels (works well on this synthetic set)\n",
    "    return np.array([\n",
    "        day_sum.mean(), day_sum.std(),\n",
    "        day_p95.mean(), day_max.mean(),\n",
    "        (vals>1.2).sum(axis=1).mean(),\n",
    "        (vals>2.0).sum(axis=1).mean()\n",
    "    ])\n",
    "\n",
    "def ev_feature_row(pdf):\n",
    "    wd = pdf[~pdf[\"is_weekend\"]]\n",
    "    we = pdf[pdf[\"is_weekend\"]]\n",
    "    f_wd = window_stats(wd, EV_WD_HOURS)\n",
    "    f_we = window_stats(we, EV_WE_HOURS)\n",
    "    # evening vs midday contrast (adds scale invariance)\n",
    "    eve = pdf.loc[pdf[\"hour\"].isin([20,21,22,23,0,1]), \"residual_kWh\"].mean()\n",
    "    day = pdf.loc[pdf[\"hour\"].isin([10,11,12,13,14,15]), \"residual_kWh\"].mean()\n",
    "    contrast = 0.0 if not np.isfinite(eve - day) else (eve - day)\n",
    "    return np.concatenate([f_wd, f_we, [contrast]])\n",
    "\n",
    "# Build feature matrix\n",
    "rows, ids = [], []\n",
    "for hid, pdf in df.groupby(\"household_id\"):\n",
    "    rows.append(ev_feature_row(pdf))\n",
    "    ids.append(hid)\n",
    "X = np.vstack(rows)\n",
    "sc = StandardScaler()\n",
    "Xz = sc.fit_transform(X)\n",
    "\n",
    "# KMeans(2) and pick the EV label by higher evening contrast (last feature)\n",
    "km = KMeans(n_clusters=2, n_init=50, random_state=42).fit(Xz)\n",
    "labels = km.labels_\n",
    "grp_eve = [X[labels==g][:,-1].mean() if (labels==g).any() else -1e9 for g in [0,1]]\n",
    "ev_label = int(np.argmax(grp_eve))\n",
    "ev_owner = {hid: (lab==ev_label) for hid,lab in zip(ids, labels)}\n",
    "df[\"is_ev_owner\"] = df[\"household_id\"].map(ev_owner)\n",
    "print(\"EV owners detected:\", int(df.groupby(\"household_id\")[\"is_ev_owner\"].first().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4180e4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EV owner share: 27.5%\n",
      "EV WD top-3 hours: hour\n",
      "22    1.219794\n",
      "21    1.219478\n",
      "23    1.208143\n",
      "Name: residual_kWh, dtype: float64\n",
      "EV WE top-3 hours: hour\n",
      "17    1.195985\n",
      "23    1.192592\n",
      "18    1.179850\n",
      "Name: residual_kWh, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 1) EV owners share (10–40% plausible depending on synthetic set)\n",
    "ev_share = np.mean(list(ev_owner.values()))\n",
    "print(f\"EV owner share: {100*ev_share:.1f}%\")\n",
    "\n",
    "# 2) Peak timing check: EV owners should peak at WD evenings and WE midday\n",
    "ev_df = df[df[\"is_ev_owner\"]]\n",
    "wd = ev_df[~ev_df[\"is_weekend\"]].groupby(\"hour\")[\"residual_kWh\"].mean()\n",
    "we = ev_df[ ev_df[\"is_weekend\"]].groupby(\"hour\")[\"residual_kWh\"].mean()\n",
    "print(\"EV WD top-3 hours:\", wd.sort_values(ascending=False).head(3))\n",
    "print(\"EV WE top-3 hours:\", we.sort_values(ascending=False).head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af105550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coldness proxy: average residual across all households per day, normalized\n",
    "daily_mean_resid = df.groupby(\"date\")[\"residual_kWh\"].mean()\n",
    "cmin, cmax = daily_mean_resid.min(), daily_mean_resid.max()\n",
    "coldness = (daily_mean_resid - cmin) / (cmax - cmin + 1e-12)\n",
    "coldness.name = \"coldness_idx\"\n",
    "df = df.merge(coldness.to_frame(), left_on=\"date\", right_index=True, how=\"left\")\n",
    "\n",
    "# Learn OTHER template from warm weekdays (exclude May to avoid “no heating” artefacts)\n",
    "warm_cut = coldness.quantile(0.35)\n",
    "other_source = df[(~df[\"is_weekend\"]) & (df[\"month\"]!=MAY_MONTH) & (df[\"coldness_idx\"]<=warm_cut)]\n",
    "mat_other = safe_pivot(other_source, \"residual_kWh\").dropna()\n",
    "other_template = mat_other.median(axis=0).values\n",
    "other_template = other_template / (other_template.sum() + 1e-12)\n",
    "\n",
    "# Learn HEATING template from cold weekdays (top quartile of coldness)\n",
    "cold_cut = coldness.quantile(0.75)\n",
    "heat_source = df[(~df[\"is_weekend\"]) & (df[\"coldness_idx\"]>=cold_cut)]\n",
    "mat_heat = safe_pivot(heat_source, \"residual_kWh\").dropna()\n",
    "# isolate heating-ish shape by removing a scaled copy of \"other\" first (simple projection)\n",
    "if not mat_heat.empty:\n",
    "    H_raw = mat_heat.median(axis=0).values\n",
    "    # Remove daytime “other” leakage with a nonnegative scalar alpha\n",
    "    alpha = max(0.0, min(1.0, np.dot(H_raw, other_template)/np.dot(other_template, other_template)))\n",
    "    H_shape = np.clip(H_raw - alpha*other_template, 0, None)\n",
    "else:\n",
    "    H_shape = np.zeros(24)\n",
    "\n",
    "# if heating shape too sparse, fall back to AM/PM bumps\n",
    "if H_shape.sum() < 1e-6:\n",
    "    H_shape = np.zeros(24)\n",
    "    for h in HEAT_MORN: H_shape[h] += 1\n",
    "    for h in HEAT_EVE:  H_shape[h] += 1.2\n",
    "heating_template = H_shape / (H_shape.sum() + 1e-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf2655a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved submission_stepwise.csv\n"
     ]
    }
   ],
   "source": [
    "def ev_preallocate(day_resid, is_weekend, is_owner):\n",
    "    ev = np.zeros(24, dtype=float)\n",
    "    if not is_owner:\n",
    "        return ev\n",
    "    win = EV_WE_HOURS if is_weekend else EV_WD_HOURS\n",
    "    vals = np.array([day_resid[h] for h in win], float)\n",
    "    if len(vals)==0:\n",
    "        return ev\n",
    "    # percentile threshold per-day + floor\n",
    "    thr = max(np.percentile(vals, 85), 1.1)  # tweak 80–90 and 1.0–1.4 if needed\n",
    "    for h in win:\n",
    "        excess = day_resid[h] - thr\n",
    "        if excess > 0:\n",
    "            ev[h] = excess\n",
    "    return np.minimum(ev, day_resid)\n",
    "\n",
    "rows = []\n",
    "for (hid, day), g in df.groupby([\"household_id\",\"date\"]):\n",
    "    is_we = bool(g[\"is_weekend\"].iloc[0])\n",
    "    ao    = float(baseline_map[hid])\n",
    "    y_tot = g.set_index(\"hour\")[\"consumption_kWh\"].reindex(range(24)).values\n",
    "    y_res = np.maximum(y_tot - ao, 0.0)\n",
    "\n",
    "    # 1) EV spikes first\n",
    "    is_owner = bool(df.loc[g.index, \"is_ev_owner\"].iloc[0])\n",
    "    ev_part = ev_preallocate(y_res, is_we, is_owner)\n",
    "    rem = np.maximum(y_res - ev_part, 0.0)\n",
    "\n",
    "    # 2) NNLS for [heating, other] on remainder\n",
    "    #    Scale heating by coldness (day-level) AFTER fit (amplitude), keep shape fixed\n",
    "    cold = float(df.loc[g.index, \"coldness_idx\"].iloc[0])\n",
    "    X = np.vstack([heating_template, other_template]).T  # (24x2), columns sum to 1\n",
    "    coeffs, _ = nnls(X, rem)\n",
    "    heat = coeffs[0] * heating_template\n",
    "    oth  = coeffs[1] * other_template\n",
    "\n",
    "    # 3) No heating in May\n",
    "    if pd.Timestamp(day).month == MAY_MONTH:\n",
    "        heat[:] = 0.0\n",
    "\n",
    "    # 4) Per-hour conservation\n",
    "    var = heat + ev_part + oth\n",
    "    need = np.maximum(y_tot - ao, 0.0)\n",
    "    scale = np.divide(need, var, out=np.ones_like(need), where=var>1e-9)\n",
    "    heat = np.clip(heat * scale, 0, None)\n",
    "    ev   = np.clip(ev_part * scale, 0, None)\n",
    "    oth  = np.clip(oth * scale, 0, None)\n",
    "\n",
    "    # final small rebalance into \"other\"\n",
    "    recon = ao + heat + ev + oth\n",
    "    fix   = (y_tot - recon)\n",
    "    oth   = np.clip(oth + fix, 0, None)\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"household_id\": hid,\n",
    "        \"date\": day,\n",
    "        \"hour\": np.arange(24),\n",
    "        \"heating_kWh\": heat,\n",
    "        \"ev_kWh\": ev,\n",
    "        \"always_on_kWh\": np.full(24, ao),\n",
    "        \"other_kWh\": oth\n",
    "    })\n",
    "    rows.append(out)\n",
    "\n",
    "pred = pd.concat(rows, ignore_index=True)\n",
    "pred[\"timestamp\"] = pd.to_datetime(pred[\"date\"].astype(str)) + pd.to_timedelta(pred[\"hour\"], unit=\"h\")\n",
    "pred[\"kaggle_id\"] = pred[\"household_id\"].astype(str) + \"_\" + pred[\"timestamp\"].dt.strftime(\"%Y-%m-%d %H\")\n",
    "\n",
    "submit = pred[[\"kaggle_id\",\"heating_kWh\",\"ev_kWh\",\"always_on_kWh\",\"other_kWh\"]].sort_values(\"kaggle_id\")\n",
    "submit.to_csv(\"submission_stepwise.csv\", index=False)\n",
    "print(\"Saved submission_stepwise.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30595240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abs diff mean: 0.011111069972826095   95p: 0.08344\n",
      "Residual vs coldness correlation: 0.671\n",
      "Weekend/Weekday ratios:\n",
      " heating_kWh      1.011676\n",
      "ev_kWh           0.888260\n",
      "always_on_kWh    1.000000\n",
      "other_kWh        0.989902\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Hourly conservation check\n",
    "chk = df[[\"kaggle_id\",\"consumption_kWh\"]].merge(submit, on=\"kaggle_id\", how=\"left\")\n",
    "tot_pred = chk[[\"heating_kWh\",\"ev_kWh\",\"always_on_kWh\",\"other_kWh\"]].sum(axis=1)\n",
    "diff = chk[\"consumption_kWh\"] - tot_pred\n",
    "print(\"Abs diff mean:\", diff.abs().mean(), \"  95p:\", diff.abs().quantile(0.95))\n",
    "\n",
    "# Residual vs coldness correlation — target ~0 to +0.2 (previously 0.64)\n",
    "df_pred = df.merge(submit.rename(columns={\n",
    "    \"heating_kWh\":\"pred_heat\",\"ev_kWh\":\"pred_ev\",\"always_on_kWh\":\"pred_ao\",\"other_kWh\":\"pred_other\"\n",
    "}), on=\"kaggle_id\", how=\"left\")\n",
    "df_pred[\"reconstructed\"] = df_pred[[\"pred_heat\",\"pred_ev\",\"pred_ao\",\"pred_other\"]].sum(axis=1)\n",
    "df_pred[\"resid_bias\"] = df_pred[\"consumption_kWh\"] - df_pred[\"reconstructed\"]\n",
    "daily_bias = df_pred.groupby(\"date\")[\"resid_bias\"].mean().to_frame().merge(\n",
    "    df_pred.groupby(\"date\")[\"coldness_idx\"].first(), left_index=True, right_index=True)\n",
    "corr = np.corrcoef(daily_bias[\"resid_bias\"], daily_bias[\"coldness_idx\"])[0,1]\n",
    "print(\"Residual vs coldness correlation:\", round(float(corr), 3))\n",
    "\n",
    "# Weekend/weekday ratios (sanity)\n",
    "subv = submit.copy()\n",
    "subv[\"timestamp\"] = pd.to_datetime(subv[\"kaggle_id\"].str.split(\"_\").str[1])\n",
    "subv[\"dow\"] = subv[\"timestamp\"].dt.dayofweek\n",
    "subv[\"is_weekend\"] = subv[\"dow\"] >= 5\n",
    "wk = subv[~subv[\"is_weekend\"]].mean(numeric_only=True)\n",
    "we = subv[ subv[\"is_weekend\"]].mean(numeric_only=True)\n",
    "print(\"Weekend/Weekday ratios:\\n\", (we/wk)[[\"heating_kWh\",\"ev_kWh\",\"always_on_kWh\",\"other_kWh\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cacb980",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v4/lzkm2zzd3gb0q8rqggx5drch0000gn/T/ipykernel_13921/1557820549.py:42: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  baseline_map = df.groupby(\"household_id\").apply(pick_baseline).to_dict()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved submission_residual_dict_wdwe_softEV_ensemble.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Improved Data-driven Residual Dictionary (Weekday/Weekend + Soft EV gate + Ensemble)\n",
    "# ============================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.optimize import nnls\n",
    "\n",
    "RNG_SEEDS = [17, 42, 73]       # try 3–5 seeds for a small ensemble\n",
    "N_CLUSTERS = 8                 # 6→8 often helps\n",
    "NIGHT_HOURS = [2,3,4,5]\n",
    "EV_WD_HOURS = [20,21,22,23,0,1]\n",
    "EV_WE_HOURS = [11,12,13,14,15,16]\n",
    "\n",
    "# -----------------------------\n",
    "# Load + basic indexing\n",
    "# -----------------------------\n",
    "df = pd.read_csv(\"kaggle_clustering_student_version.csv\")\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "df[\"date\"]      = df[\"timestamp\"].dt.date\n",
    "df[\"hour\"]      = df[\"timestamp\"].dt.hour\n",
    "df[\"dow\"]       = df[\"timestamp\"].dt.dayofweek\n",
    "df[\"is_weekend\"]= df[\"dow\"] >= 5\n",
    "df[\"kaggle_id\"] = df[\"household_id\"].astype(str) + \"_\" + df[\"timestamp\"].dt.strftime(\"%Y-%m-%d %H\")\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 1 — Baseline (always-on) per household from night quantile\n",
    "# -----------------------------\n",
    "BASELINE_QS = [0.10, 0.15, 0.20]  # tiny grid; we pick the one minimizing *night* residual MSE\n",
    "\n",
    "def pick_baseline(pdf):\n",
    "    best = (1e9, 0.15, 0.1)\n",
    "    nights = pdf.loc[pdf[\"hour\"].isin(NIGHT_HOURS), \"consumption_kWh\"].values\n",
    "    if len(nights) < 4: nights = pdf[\"consumption_kWh\"].values\n",
    "    for q in BASELINE_QS:\n",
    "        b = max(0.01, float(np.quantile(nights, q)))\n",
    "        err = np.mean(np.clip(nights - b, 0, None)**2)\n",
    "        if err < best[0]: best = (err, q, b)\n",
    "    return best[2]\n",
    "\n",
    "baseline_map = df.groupby(\"household_id\").apply(pick_baseline).to_dict()\n",
    "df[\"always_on_kWh\"] = df[\"household_id\"].map(baseline_map)\n",
    "df[\"residual_kWh\"]  = (df[\"consumption_kWh\"] - df[\"always_on_kWh\"]).clip(lower=0)\n",
    "\n",
    "# System “coldness” proxy (helps diagnostics / optional penalties)\n",
    "daily_mean_resid = df.groupby(\"date\")[\"residual_kWh\"].mean()\n",
    "cmin, cmax = daily_mean_resid.min(), daily_mean_resid.max()\n",
    "coldness = ((daily_mean_resid - cmin) / (cmax - cmin + 1e-12)).rename(\"coldness_idx\")\n",
    "df = df.merge(coldness.to_frame(), left_on=\"date\", right_index=True, how=\"left\")\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 2 — EV owner detection (scale-robust)\n",
    "# -----------------------------\n",
    "def safe_pivot(pdf, value_col=\"residual_kWh\"):\n",
    "    g = pdf.groupby([\"date\",\"hour\"], as_index=False)[value_col].mean()\n",
    "    mat = g.pivot(index=\"date\", columns=\"hour\", values=value_col).reindex(columns=range(24))\n",
    "    return mat\n",
    "\n",
    "def ev_features(pdf):\n",
    "    wd = pdf[~pdf[\"is_weekend\"]]; we = pdf[pdf[\"is_weekend\"]]\n",
    "    def window_feats(p, hours):\n",
    "        mat = safe_pivot(p)\n",
    "        if mat is None or mat.empty: return np.zeros(6)\n",
    "        vals = mat[hours].fillna(0).values\n",
    "        day_sum = vals.sum(axis=1)\n",
    "        day_p95 = np.percentile(vals, 95, axis=1)\n",
    "        day_max = vals.max(axis=1)\n",
    "        return np.array([day_sum.mean(), day_sum.std(), day_p95.mean(), day_max.mean(),\n",
    "                         (vals>1.2).sum(axis=1).mean(), (vals>2.0).sum(axis=1).mean()])\n",
    "    f_wd = window_feats(wd, EV_WD_HOURS)\n",
    "    f_we = window_feats(we, EV_WE_HOURS)\n",
    "    eve = pdf.loc[pdf[\"hour\"].isin(EV_WD_HOURS), \"residual_kWh\"].mean()\n",
    "    day = pdf.loc[pdf[\"hour\"].isin([10,11,12,13,14,15]), \"residual_kWh\"].mean()\n",
    "    contrast = 0.0 if not np.isfinite(eve-day) else (eve-day)\n",
    "    return np.concatenate([f_wd, f_we, [contrast]])\n",
    "\n",
    "ev_rows, hh_ids = [], []\n",
    "for hid, pdf in df.groupby(\"household_id\"):\n",
    "    ev_rows.append(ev_features(pdf)); hh_ids.append(hid)\n",
    "X = np.vstack(ev_rows)\n",
    "Xz = StandardScaler().fit_transform(X)\n",
    "lab = KMeans(n_clusters=2, n_init=40, random_state=42).fit_predict(Xz)\n",
    "grp_eve_contrast = [X[lab==g][:,-1].mean() if (lab==g).any() else -1e9 for g in [0,1]]\n",
    "ev_label = int(np.argmax(grp_eve_contrast))\n",
    "ev_owner = {hid: (l==ev_label) for hid,l in zip(hh_ids, lab)}\n",
    "df[\"is_ev_owner\"] = df[\"household_id\"].map(ev_owner)\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 3 — Dictionaries on residuals (weekday/weekend) + soft EV gate + ensemble\n",
    "# -----------------------------\n",
    "def collect_residual_days(pdf):\n",
    "    mat = safe_pivot(pdf, \"residual_kWh\").dropna(how=\"any\")\n",
    "    return mat.values  # (N_days x 24)\n",
    "\n",
    "def auto_map(centroids):\n",
    "    \"\"\"Return indices (heat_idx, ev_idx, other_idx). We exclude 'flat' because we model baseline separately.\"\"\"\n",
    "    C = centroids\n",
    "    # Scoring\n",
    "    def ev_score(c): return c[20:].mean() - c[:10].mean()\n",
    "    def heat_score(c): return c[6:10].mean() + c[17:22].mean() - c.mean()\n",
    "    def other_score(c): return c[9:16].mean()  # daytime\n",
    "    # Avoid “flat” profiles (std very low)\n",
    "    stds = C.std(axis=1)\n",
    "    order = np.argsort(stds)  # low→high\n",
    "    flat_candidates = set(order[:max(1, len(order)//N_CLUSTERS)])  # 1 low-std cluster disfavored\n",
    "    idxs = list(range(C.shape[0]))\n",
    "    # choose distinct maxima\n",
    "    ev_idx   = max(idxs, key=lambda i: (ev_score(C[i]), -stds[i]))\n",
    "    heat_idx = max([i for i in idxs if i!=ev_idx], key=lambda i: (heat_score(C[i]), -stds[i]))\n",
    "    others   = [i for i in idxs if i not in (ev_idx, heat_idx)]\n",
    "    other_idx= max(others, key=lambda i: (other_score(C[i]), -stds[i])) if others else heat_idx\n",
    "    return heat_idx, ev_idx, other_idx\n",
    "\n",
    "def learn_basis(days_24xN, seed):\n",
    "    if days_24xN.shape[0] < 40:  # fallback small sample\n",
    "        # generic shapes (normalized)\n",
    "        heat = np.zeros(24);  heat[[6,7,8,9]] += 1; heat[[17,18,19,20,21]] += 1.2\n",
    "        ev   = np.zeros(24);  ev[EV_WD_HOURS] += 1\n",
    "        other= np.zeros(24);  other[[7,8,9,17,18]] += 1; other += 0.3\n",
    "        H = heat/heat.sum(); E = ev/ev.sum(); O = other/other.sum()\n",
    "        return np.vstack([H,E,O]).T  # (24x3)\n",
    "    # standardize each day by its sum (shape over magnitude)\n",
    "    X = days_24xN / (days_24xN.sum(axis=1, keepdims=True) + 1e-12)\n",
    "    km = KMeans(n_clusters=N_CLUSTERS, n_init=30, random_state=seed)\n",
    "    labels = km.fit_predict(X)\n",
    "    C = km.cluster_centers_  # (k x 24), already normalized to ~unit sum\n",
    "    # auto-map to (heating, EV, other)\n",
    "    h_idx, e_idx, o_idx = auto_map(C)\n",
    "    B = np.vstack([C[h_idx], C[e_idx], C[o_idx]]).T  # (24x3)\n",
    "    # re-normalize columns exactly\n",
    "    B = B / (B.sum(axis=0, keepdims=True) + 1e-12)\n",
    "    return B\n",
    "\n",
    "# Learn two bases per seed: weekday + weekend\n",
    "bases = []  # list of dicts: {\"wd\": 24x3, \"we\": 24x3}\n",
    "for seed in RNG_SEEDS:\n",
    "    wd_days = collect_residual_days(df[~df[\"is_weekend\"]])\n",
    "    we_days = collect_residual_days(df[ df[\"is_weekend\"]])\n",
    "    B_wd = learn_basis(wd_days, seed)\n",
    "    B_we = learn_basis(we_days, seed)\n",
    "    bases.append({\"wd\": B_wd, \"we\": B_we})\n",
    "\n",
    "# Soft EV gate: column multiplier for EV for non-owners (0.3–0.6 works well)\n",
    "EV_PENALTY = 0.4\n",
    "\n",
    "def fit_one_day(day_df, base_wd, base_we, is_ev_owner, baseline):\n",
    "    is_we = bool(day_df[\"is_weekend\"].iloc[0])\n",
    "    B = base_we if is_we else base_wd         # (24x3) columns = [heat, ev, other]\n",
    "    # apply soft penalty for EV if non-owner\n",
    "    P = np.array([1.0, 1.0 if is_ev_owner else EV_PENALTY, 1.0])\n",
    "    X = (B * P).T.T                            # column scaling\n",
    "    # target residual\n",
    "    y_total = day_df.set_index(\"hour\")[\"consumption_kWh\"].reindex(range(24)).values\n",
    "    y = np.maximum(y_total - baseline, 0.0)\n",
    "    # NNLS\n",
    "    a, _ = nnls(X, y)\n",
    "    comps = B * a  # use *unpenalized* basis to keep original shapes\n",
    "    # per-hour scale so residual sums match exactly\n",
    "    var = comps.sum(axis=1)\n",
    "    scale = np.divide(y, var, out=np.ones_like(y), where=var>1e-9)\n",
    "    comps = np.clip(comps * scale[:,None], 0, None)\n",
    "    # return heating, ev, other\n",
    "    return comps[:,0], comps[:,1], comps[:,2]\n",
    "\n",
    "# -----------------------------\n",
    "# Inference with ensemble + hourwise conservation\n",
    "# -----------------------------\n",
    "pieces = []\n",
    "for (hid, day), g in df.groupby([\"household_id\",\"date\"]):\n",
    "    baseline = baseline_map[hid]\n",
    "    evflag   = bool(ev_owner.get(hid, False))\n",
    "    # ensemble the three bases\n",
    "    heats = []; evs = []; others = []\n",
    "    for b in bases:\n",
    "        h, e, o = fit_one_day(g, b[\"wd\"], b[\"we\"], evflag, baseline)\n",
    "        heats.append(h); evs.append(e); others.append(o)\n",
    "    heat  = np.mean(heats, axis=0)\n",
    "    ev    = np.mean(evs, axis=0)\n",
    "    other = np.mean(others, axis=0)\n",
    "    ao    = np.full(24, baseline)\n",
    "\n",
    "    # strict hour-wise conservation\n",
    "    total = g.set_index(\"hour\")[\"consumption_kWh\"].reindex(range(24)).values\n",
    "    var   = heat + ev + other\n",
    "    need  = np.maximum(total - ao, 0.0)\n",
    "    scl   = np.divide(need, var, out=np.ones_like(need), where=var>1e-9)\n",
    "    heat  = np.clip(heat * scl, 0, None)\n",
    "    ev    = np.clip(ev   * scl, 0, None)\n",
    "    other = np.clip(other* scl, 0, None)\n",
    "    # tiny numeric fix\n",
    "    recon = ao + heat + ev + other\n",
    "    other = np.clip(other + (total - recon), 0, None)\n",
    "\n",
    "    df_out = pd.DataFrame({\n",
    "        \"household_id\": hid,\n",
    "        \"date\": day,\n",
    "        \"hour\": np.arange(24),\n",
    "        \"heating_kWh\": heat,\n",
    "        \"ev_kWh\": ev,\n",
    "        \"always_on_kWh\": ao,\n",
    "        \"other_kWh\": other\n",
    "    })\n",
    "    pieces.append(df_out)\n",
    "\n",
    "pred = pd.concat(pieces, ignore_index=True)\n",
    "pred[\"timestamp\"] = pd.to_datetime(pred[\"date\"].astype(str)) + pd.to_timedelta(pred[\"hour\"], unit=\"h\")\n",
    "pred[\"kaggle_id\"] = pred[\"household_id\"].astype(str) + \"_\" + pred[\"timestamp\"].dt.strftime(\"%Y-%m-%d %H\")\n",
    "\n",
    "submit = pred[[\"kaggle_id\",\"heating_kWh\",\"ev_kWh\",\"always_on_kWh\",\"other_kWh\"]].sort_values(\"kaggle_id\")\n",
    "submit.to_csv(\"submission_residual_dict_wdwe_softEV_ensemble.csv\", index=False)\n",
    "print(\"Saved submission_residual_dict_wdwe_softEV_ensemble.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47da3796",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4es_exercises",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
